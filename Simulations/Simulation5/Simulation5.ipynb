{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Nikki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nikki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the text library\n",
    "txtLibraryPath = '/Users/Nikki/Dropbox/UNC/Causal NLP/Reback_TxtLibrary/Reback_Project Tech Support Text Message Library_NF.xlsx'\n",
    "txtLibrary = pd.read_excel(txtLibraryPath, \n",
    "                           sheet_name = \"Library\",\n",
    "                          skiprows = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Tidy up the text message library \n",
    "\n",
    "* Remove the skipped lines\n",
    "* Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtLibrary.columns = ['txtID', 'txt']\n",
    "txtLibrary = txtLibrary.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "* Make all words lower case\n",
    "* Remove numbers\n",
    "* Remove single character words (i.e., \"a\", \"i\", \"n\")\n",
    "* Remove stop words\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case\n",
    "txtLibrary.txt = txtLibrary.txt.str.lower()\n",
    "\n",
    "# Remove single character words\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('\\\\b[a-z]\\\\b', \"\")\n",
    "\n",
    "# Remove numbers\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[0-9]', \"\").tolist()\n",
    "\n",
    "# Remove punctuation\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[^\\\\w\\\\s]', \"\")\n",
    "\n",
    "# Remove the fill-in-the-blank blanks\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[_]+', \"\")\n",
    "\n",
    "# Remove extra white space\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[\\\\s]+', \" \")\n",
    "\n",
    "# Make a list containing the text message strings\n",
    "txtMsgStringList = txtLibrary.txt.tolist()\n",
    "\n",
    "# Tokenize\n",
    "txtMsgTokens = [nltk.tokenize.word_tokenize(x) for x in txtMsgStringList]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words  = set(nltk.corpus.stopwords.words('english'))\n",
    "txtMsgTokens = [[w for w in text if not w in stop_words] for text in txtMsgTokens]\n",
    "\n",
    "# Stem tokens\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "txtMsgTokensStemmed = [[porter.stem(word) for word in text] for text in txtMsgTokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the token list\n",
    "txtMsgTokensStemmed_flat = [y for x in txtMsgTokensStemmed for y in x]\n",
    "\n",
    "# Convert the flattened token list into an array\n",
    "txtMsgTokensStemmed_flatArray = np.array(txtMsgTokensStemmed_flat)\n",
    "\n",
    "# Get the unique stemmed tokens\n",
    "uniqueStemmedTokens = np.unique(txtMsgTokensStemmed_flatArray)\n",
    "\n",
    "# Count the total number of unique stemmed tokens\n",
    "N_stemmedTokens = uniqueStemmedTokens.size\n",
    "\n",
    "# Create an list of text message arrays\n",
    "txtMsgTokensStemmed_array = [np.array(x) for x in txtMsgTokensStemmed]\n",
    "\n",
    "# Count up the frequency of each stemmed word in the texts\n",
    "countOfStemmedTokensInTextMsg = []\n",
    "for x in txtMsgTokensStemmed_array:\n",
    "    countOfStemmedTokensInTextMsg.append([sum(x == y) for y in uniqueStemmedTokens])\n",
    "        \n",
    "# Count up the number of stemmed tokens in each text message\n",
    "N_stemmedTokensInTextMsg = [len(x) for x in txtMsgTokensStemmed]\n",
    "\n",
    "# Term frequency\n",
    "# This is tf\n",
    "tf = []\n",
    "for x, y in zip(countOfStemmedTokensInTextMsg, N_stemmedTokensInTextMsg):\n",
    "    tf.append([z/y for z in x])\n",
    "\n",
    "# Count of the number of occurrences of each word in the corpus (count of number of texts with word)\n",
    "# This is df = document frequency\n",
    "occurrenceOfStemmedTokensInCorpus = []\n",
    "counter = 0\n",
    "for x in uniqueStemmedTokens:\n",
    "    for y in txtMsgTokensStemmed_array:\n",
    "        if(sum(y == x) > 0): \n",
    "            counter += 1\n",
    "    occurrenceOfStemmedTokensInCorpus.append(counter)\n",
    "    counter = 0\n",
    "    \n",
    "# Add 1 to occurrence counts\n",
    "occurrenceOfStemmedTokensInCorpus_smoothedArray = np.array(occurrenceOfStemmedTokensInCorpus) +1\n",
    "\n",
    "# Number of documents in the coprus\n",
    "N_textMessages = len(txtMsgTokensStemmed)\n",
    "\n",
    "# IDF = log(N/(df + 1))\n",
    "idf = np.log(N_textMessages/occurrenceOfStemmedTokensInCorpus_smoothedArray)\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = []\n",
    "for x in tf:\n",
    "    tf_idf.append(x*idf)\n",
    "    \n",
    "tf_idf_array = np.vstack(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the participants and their outcomes\n",
    "\n",
    "### Parameters for ppt creation\n",
    "\n",
    "* Set the seed\n",
    "* R is the number of participants\n",
    "* `mu_true` is the list of true means for the outcomes\n",
    "* `sigma_true` is a list of the true sds for the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "random.seed(1001)\n",
    "\n",
    "# Number of participants\n",
    "R = len(txtLibrary.txtID)\n",
    "\n",
    "# Extract the true topics\n",
    "txtLibrary['topic'] = txtLibrary.txtID.astype(str).str[0]\n",
    "\n",
    "# True means and standard deviations\n",
    "trueTopicList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'P']\n",
    "trueMeansList = [-20, -15, -10, -5, 0, 5, 10, 15, 20]\n",
    "trueSDsList = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "trueTopicsAndMeansDict = dict(zip(trueTopicList, trueMeansList))\n",
    "trueTopicsAndSDsDict = dict(zip(trueTopicList, trueSDsList))\n",
    "txtLibrary['true_mean'] = txtLibrary.topic.map(trueTopicsAndMeansDict)\n",
    "txtLibrary['sigma_true'] = txtLibrary.topic.map(trueTopicsAndSDsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create participant outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtLibrary['Y'] = 0\n",
    "txtLibrary = txtLibrary.set_index('txtID')\n",
    "    \n",
    "for txtid in txtLibrary.index:\n",
    "    txtLibrary.at[txtid, 'Y'] = np.random.normal(txtLibrary.at[txtid, 'true_mean'], \n",
    "                                                 txtLibrary.at[txtid, 'sigma_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      txt topic  true_mean  \\\n",
      "txtID                                                                        \n",
      "A1a001   meth will mess with your hiv be good to yourself     A        -20   \n",
      "A1a002  some strains are drug resistant you are worth ...     A        -20   \n",
      "A1a003                        poz partying not good combo     A        -20   \n",
      "A1a004  poz using not good cocktail you deserve to be ...     A        -20   \n",
      "A1a005  tina ain your friend if she makes you forget y...     A        -20   \n",
      "...                                                   ...   ...        ...   \n",
      "Post1            hi just reminder your followup visit is      P         20   \n",
      "Post2   hi your followup visit is couple weeks away it...     P         20   \n",
      "Post3   hi hope all is well your appt is scheduled for...     P         20   \n",
      "Post4                  your followup appt is see you then     P         20   \n",
      "Post5   your appt is coming up it set for call us if t...     P         20   \n",
      "\n",
      "        sigma_true   Y  \n",
      "txtID                   \n",
      "A1a001           1 -21  \n",
      "A1a002           1 -19  \n",
      "A1a003           1 -18  \n",
      "A1a004           1 -21  \n",
      "A1a005           1 -20  \n",
      "...            ...  ..  \n",
      "Post1            1  20  \n",
      "Post2            1  20  \n",
      "Post3            1  20  \n",
      "Post4            1  21  \n",
      "Post5            1  20  \n",
      "\n",
      "[660 rows x 5 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
