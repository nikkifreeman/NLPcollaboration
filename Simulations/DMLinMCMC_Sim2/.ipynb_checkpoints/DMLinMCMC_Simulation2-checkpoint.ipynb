{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Topic modeling: Short text topic modeling (Ref: X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random \n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "# Load the document-text matrix\n",
    "DTMPath = '/Users/Nikki/Dropbox/UNC/Causal NLP/Reback_TxtLibrary/RebackDTM.csv'\n",
    "fullDTM = pd.read_csv(DTMPath, index_col= 'msgID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset on the text messages for this simulation\n",
    "DTM = fullDTM.loc['A1a001':'B3b017', :]\n",
    "\n",
    "# Remove the columns that are only 0s (those words don't show up in the subset of text messages)\n",
    "DTM = DTM.loc[:, DTM.sum() > 0] #112 text messages and 195 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ppts and their outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for ppt creation\n",
    "\n",
    "* Set the seed\n",
    "* R is the number of participants\n",
    "* mu_true is a list of true means for the outcomes\n",
    "* sigma_true is a list of true sds for the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "random.seed(1000)\n",
    "\n",
    "# Number of participants\n",
    "R = 60\n",
    "\n",
    "# True means\n",
    "mu_true = [0, 10]\n",
    "\n",
    "# True sds\n",
    "sigma_true = [.25, .25]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create participants and assign text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ppt ids\n",
    "ppt = list(range(1, R+1))\n",
    "\n",
    "# Get the text message assignments\n",
    "assignments = np.random.choice(list(DTM.index), size = R, replace = False)\n",
    "\n",
    "# Put the ppts and their test assignments into a pandas dataframe\n",
    "pptDF = pd.DataFrame(list(zip(ppt, assignments)), \n",
    "               columns =['pptID', 'msgID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create participant outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the outcomes\n",
    "pptDF['topicA'] = pptDF['msgID'].str.contains('^A')\n",
    "pptDF['Y'] = pptDF['topicA']. apply(lambda x: np.random.normal(mu_true[0], sigma_true[0]) if x == True else np.random.normal(mu_true[1], sigma_true[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sparse matrix-like structure for the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the words to an id (1:V), store in a dictionary\n",
    "V = len(DTM.columns)\n",
    "wordDict = dict(zip(DTM.columns, range(1, V+1)))\n",
    "\n",
    "# Assign the messages to an id (1:D), store in a dictionary\n",
    "D = len(DTM.index)\n",
    "msgDict = dict(zip(DTM.index, range(1, D+1)))\n",
    "\n",
    "# Make the DTM dataframe long, make the message ids their own variable (instead of the index) \n",
    "DTMLong = pd.melt(DTM.reset_index(), id_vars = 'msgID')\n",
    "DTMLong = DTMLong[DTMLong.value > 0] \n",
    "\n",
    "# Add the word numbers to the text message long df\n",
    "DTMLong['wordNum'] = DTMLong['variable'].map(wordDict)\n",
    "DTMLong = DTMLong.rename(columns = {'variable':'word'})\n",
    "\n",
    "# Add the message numbers to the text message long df\n",
    "DTMLong['msgNum'] = DTMLong['msgID'].map(msgDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the labeled and unlabeled messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the labeled message ids \n",
    "labeledMsgs = pd.DataFrame(pptDF[['msgID']])\n",
    "labeledMsgs['labeled'] = 1\n",
    "\n",
    "# Use the labeled Msgs dataframe to identify the labeled messages in DTMLong\n",
    "# Merge the labeled messages df to the long dtm data frame\n",
    "DTMLong = DTMLong.merge(labeledMsgs, how = 'left', left_on = 'msgID', right_on = 'msgID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics\n",
    "K = 2\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = np.repeat(1, K)\n",
    "delta = np.repeat(1, V)\n",
    "Sigma_0_inv = np.identity(K)\n",
    "beta_0 = np.array([[1], [1]])\n",
    "nu_0 = 3\n",
    "sigma_0 = 0.5\n",
    "\n",
    "# Number of samples\n",
    "L = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_tildek_noD_v(v, k_tilde, wordCount, topicList, wordList):\n",
    "    out = sum(wordCount[(topicList == k_tilde) & (wordList == v)])\n",
    "    return(out)\n",
    "\n",
    "def n_tildek_noD(k_tilde, wordCount, topicList):\n",
    "    out = sum(wordCount[topicList == k_tilde])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists of the labeled and unlabeled messages\n",
    "labeledMsgsList = labeledMsgs['msgID'].tolist()\n",
    "unlabeledMsgsList = DTMLong.msgID[DTMLong['labeled'].isnull()].unique().tolist()\n",
    "\n",
    "# Assign initial topics\n",
    "initTopics = np.random.choice(range(1, K+1), \n",
    "                                    size = len(labeledMsgsList) + len(unlabeledMsgsList), \n",
    "                                    replace = True)\n",
    "msgAndTopicCurrent = dict(zip(DTMLong.msgID.unique(), initTopics))\n",
    "\n",
    "## Add initial topics to the DTMLong\n",
    "DTMLong['topic'] = DTMLong['msgID'].map(msgAndTopicCurrent)\n",
    "\n",
    "## Add initial topics to the pptDF\n",
    "pptDF['topic'] = pptDF['msgID'].map(msgAndTopicCurrent)\n",
    "\n",
    "# Place to save the topic draws\n",
    "Zchain = pd.DataFrame()\n",
    "pptChain = pptDF\n",
    "# Place to save the regression parameters\n",
    "betaChain = []\n",
    "sigma2 = [np.array([1])]\n",
    "# Place to save the estimated estimands\n",
    "means = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapsed Gibbs sampling with embedded inference\n",
    "\n",
    "The update probabilities are given by \n",
    "\n",
    "\\begin{align*}\n",
    "    P(z_{d^\\ast} = \\tilde{k} \\vert z_{-d^\\ast}, w, \\alpha, \\beta) \\propto& \\frac{\\prod_{v = 1}^V \\prod_{i = 1}^{N_{\\tilde{k}, -d^\\ast}^v} n_{\\tilde{k}, -d^\\ast}^v + \\tilde{\\beta}_{\\tilde{k}, v} + i - 1}{\\prod_{i = 1}^{N_{d^\\ast}} n_{\\tilde{k}, -d^\\ast} + \\sum_{v = 1}^V \\beta_{v} + i - 1} \\cdot \\frac{m_{\\tilde{k}, -d^\\ast} + \\alpha_{\\tilde{k}}}{D - 1 + \\sum_{k = 1}^K \\alpha_k} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In the code, we'll refer to $\\frac{\\prod_{v = 1}^V \\prod_{i = 1}^{N_{\\tilde{k}, -d^\\ast}^v} n_{\\tilde{k}, -d^\\ast}^v + \\tilde{\\beta}_{\\tilde{k}, v} + i - 1}{\\prod_{i = 1}^{N_{d^\\ast}} n_{\\tilde{k}, -d^\\ast} + \\sum_{v = 1}^V \\beta_{v} + i - 1}$ as `a` and refer to $\\frac{m_{\\tilde{k}, -d^\\ast} + \\alpha_{\\tilde{k}}}{D - 1 + \\sum_{k = 1}^K \\alpha_k}$ as `b` in the code.\n",
    "\n",
    "Furthermore, we'll refer to $n_{\\tilde{k}, -d^\\ast}^v$ as `n_tildek_noD_v`, $n_{\\tilde{k}, -d^\\ast}$ as `n_tildek_noD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(0, L):\n",
    "    \n",
    "    ## Divide the data in to the training set\n",
    "    ### Randomly select the messages training and testing sets\n",
    "    labeledToTrain = np.random.choice(labeledMsgsList, size = round(R/2), replace = False)\n",
    "    labeledToTest = list(set(labeledMsgsList) - set(labeledToTrain))\n",
    "    unlabeledToTrain = np.random.choice(unlabeledMsgsList, size = round((D - R)/2), replace = False)\n",
    "    unlabeledToTest = list(set(unlabeledMsgsList) - set(unlabeledToTrain)) # We won't use these for inference, but need to know which in the test set are labeld and which aren't\n",
    "    trainingMsgIds = unlabeledToTrain.tolist() + labeledToTrain.tolist()\n",
    "    \n",
    "    ### Create the training set and the holdout set\n",
    "    trainingSet = DTMLong[DTMLong['msgID'].isin(trainingMsgIds)]\n",
    "    testingSet = DTMLong[DTMLong['msgID'].isin(labeledToTest)]\n",
    "    holdout = DTMLong[DTMLong['msgID'].isin(unlabeledToTest)]\n",
    "\n",
    "    ## Create training document-text  \n",
    "    trainingSetShort = trainingSet.loc[:, ['msgID', 'topic']].drop_duplicates()\n",
    "    docList_short = trainingSetShort['msgID'].to_numpy()\n",
    "    topicList_short = trainingSetShort['topic'].to_numpy()\n",
    "    \n",
    "    ## Create testing document-text-outcome\n",
    "    testingSetShort = pptDF.loc[:, ['msgID', 'topic', 'Y']]\n",
    "    testingSetShort = testingSetShort[testingSetShort['msgID'].isin(labeledToTest)]\n",
    "    \n",
    "    ### Create lists for the Gibbs part\n",
    "    docList = trainingSet['msgID'].to_numpy()\n",
    "    wordList = trainingSet['wordNum'].to_numpy()\n",
    "    topicList = trainingSet['topic'].to_numpy()\n",
    "    wordCount = trainingSet['value'].to_numpy()\n",
    "    docListPpt = pptDF['msgID'].to_numpy()\n",
    "    outcomes = pptDF['Y'].to_numpy()\n",
    "    topicListReg = pptDF['topic']\n",
    "    \n",
    "    # Calculate the regression parameter updates\n",
    "    ## Get the design matrix\n",
    "    X = pd.get_dummies(pd.Series(topicListReg), drop_first = True)\n",
    "    X['intercept'] = np.transpose(np.ones(len(X)))\n",
    "    X = X[['intercept', 2]].to_numpy()\n",
    "    \n",
    "    ## Quantities to update beta\n",
    "    m = (Sigma_0_inv @ beta_0) + (X.transpose() @ outcomes.transpose())/sigma2[l]\n",
    "    m = m[0, :]\n",
    "    V = np.linalg.inv(Sigma_0_inv + (X.transpose() @ X)/sigma2[l])\n",
    "    ## Update beta\n",
    "    betaChain.append(np.random.multivariate_normal(m, V))\n",
    "    \n",
    "    ## Quantities to update sigma^2\n",
    "    SSR_beta = outcomes.transpose()@outcomes - 2* betaChain[l].transpose() @ X.transpose() @ outcomes + betaChain[l].transpose() @ X.transpose() @ X @ betaChain[l]\n",
    "    ig_a = (nu_0 + R)/2\n",
    "    ig_b = (nu_0 * sigma_0 + SSR_beta)/2\n",
    "    # Update sigma^2\n",
    "    sigma2.append(scipy.stats.invgamma.rvs(ig_a, ig_b, size = 1))\n",
    "    \n",
    "    \n",
    "    # Gibbs updates for each document\n",
    "    for d_star in trainingMsgIds:\n",
    "        \n",
    "        # Set the topic for the d_star document to 0\n",
    "        topicList[docList == d_star] = 0\n",
    "        topicList_short[docList_short == d_star] = 0\n",
    "        \n",
    "        # Get the words in d_star\n",
    "        wordsInDstar = wordList[docList == d_star]\n",
    "        \n",
    "        probs = np.empty([K])\n",
    "\n",
    "        for k_tilde in range(1, K+1):\n",
    "            \n",
    "            # Calculate the numerator of a\n",
    "            a_numerator = 1\n",
    "            for v in wordsInDstar:\n",
    "                N = wordCount[(docList == d_star) & (wordList == v)]\n",
    "                N = int(N)\n",
    "                for i in range(1, N+1):\n",
    "                    a_numerator = a_numerator * (n_tildek_noD_v(v, k_tilde, wordCount, topicList, wordList) + delta[v-1] + i - 1)\n",
    "                \n",
    "        \n",
    "            # Calculate the denominator of a\n",
    "            a_denominator = 1\n",
    "            N = wordCount[(docList == d_star) & (wordList == v)]\n",
    "            N = int(N)\n",
    "            for i in range(1, N+1):\n",
    "                a_denominator = a_denominator * (n_tildek_noD(k_tilde, wordCount, topicList) + sum(delta) + i - 1)\n",
    "            \n",
    "            a = a_numerator/a_denominator\n",
    "        \n",
    "            # Calculate the numerator of b\n",
    "            b_numerator = sum(topicList_short == k_tilde) + alpha[k_tilde-1]\n",
    "        \n",
    "            # Calculate the denominator of b\n",
    "            b_denominator = D - 1 + sum(alpha)\n",
    "            \n",
    "            # Calculate b\n",
    "            b = b_numerator/b_denominator\n",
    "            \n",
    "            probs[k_tilde - 1] = a/b\n",
    "        \n",
    "        if d_star in labeledToTrain:\n",
    "            # Calculate the supervision probability c\n",
    "            y = outcomes[docListPpt == d_star]\n",
    "            c1 = float(scipy.stats.norm.pdf(y, loc = betaChain[-1][0], scale = sigma2[-1]))\n",
    "            c2 = float(scipy.stats.norm.pdf(y, loc = sum(betaChain[-1]), scale = sigma2[-1]))\n",
    "            c = np.array([c1, c2])\n",
    "            probs = probs * c\n",
    "\n",
    "        # Normalize the probabilities\n",
    "        probsNormalized = probs/sum(probs)\n",
    "        \n",
    "        # Draw the new topic for the d_star document\n",
    "        newTopic = np.random.choice(range(1, K+1), size = 1, replace = False, p = probsNormalized)\n",
    "        \n",
    "        # Update the topicList\n",
    "        topicList[docList == d_star] = newTopic\n",
    "        \n",
    "        # Update the pptDF\n",
    "        pptDF.at[pptDF['msgID'] == d_star, 'topic'] = newTopic\n",
    "        \n",
    "        #Update DTMLong\n",
    "        DTMLong.at[DTMLong['msgID'] == d_star, 'topic'] = newTopic\n",
    "        \n",
    "        # Update topicListShort\n",
    "        topicList_short[docList_short == d_star] = newTopic\n",
    "        \n",
    "    \n",
    "    # Save the updated topics to the Z_chain\n",
    "    Zchain = Zchain.append(pd.DataFrame({'msgID':docList, 'topic':topicList, 'iteration':l}))\n",
    "    pptChain = pptChain.append(pptDF)\n",
    "    \n",
    "    # Use the updated topics to predict topics for the training set\n",
    "    ## Predict topics for each test document\n",
    "    for testDoc in labeledToTest:\n",
    "        test_wordList = testingSet.loc[testingSet['msgID'] == testDoc, 'wordNum'].to_numpy()\n",
    "        testProbs = []\n",
    "        for k in range(1,K+1):\n",
    "            out1 = 1\n",
    "            for wordNum in test_wordList:\n",
    "                top1 = sum(wordCount[(wordList == wordNum) & (topicList == k)]) + delta[wordNum-1]\n",
    "                bottom1 = sum(wordCount[topicList == k]) + sum(delta)\n",
    "                out1 = out1 *(top1/bottom1)\n",
    "            testProbs.append(out1 * (sum(topicList_short == k) + alpha[k-1])/(D + sum(alpha)))\n",
    "        \n",
    "        # Normalize the probabilitites\n",
    "        testProbsNormalized = testProbs/sum(testProbs)\n",
    "        \n",
    "        # Draw a topic for the test document\n",
    "        test_newTopic = np.random.choice(range(1, K+1), size = 1, replace = False, p = testProbsNormalized)\n",
    "        \n",
    "        # Update the test set topic\n",
    "        testingSetShort.at[testingSetShort['msgID'] == testDoc, 'topic'] = test_newTopic\n",
    "    \n",
    "    # Calculate the estimand\n",
    "    meansForIter = testingSetShort.groupby('topic')['Y'].mean()\n",
    "    meansForIter = meansForIter.to_frame()\n",
    "    meansForIter['iter'] = l\n",
    "    means = means.append(meansForIter)\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "                \n",
    "        \n",
    "    \n",
    "            \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "pptChain.to_csv('pptChain.csv')\n",
    "Zchain.to_csv('Zchain.csv')\n",
    "means.to_csv('means.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
