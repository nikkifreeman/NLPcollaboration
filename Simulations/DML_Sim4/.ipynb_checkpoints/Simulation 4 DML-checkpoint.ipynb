{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short text topic modeling and double machine learning for causal inference and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-4f32c09c4919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# nltk.download('stopwords')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the text library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'set'"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "import math\n",
    "\n",
    "# Load the text library\n",
    "txtLibraryPath = '/Users/Nikki/Dropbox/UNC/Causal NLP/Reback_TxtLibrary/Reback_Project Tech Support Text Message Library_NF.xlsx'\n",
    "txtLibrary = pd.read_excel(txtLibraryPath, \n",
    "                           sheet_name = \"Library\",\n",
    "                          skiprows = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Tidy up the text message library \n",
    "\n",
    "* Remove the skipped lines\n",
    "* Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtLibrary.columns = ['txtID', 'txt']\n",
    "txtLibrary = txtLibrary.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "* Make all words lower case\n",
    "* Remove numbers\n",
    "* Remove single character words (i.e., \"a\", \"i\", \"n\")\n",
    "* Remove stop words\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case\n",
    "txtLibrary.txt = txtLibrary.txt.str.lower()\n",
    "\n",
    "# Remove single character words\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('\\\\b[a-z]\\\\b', \"\")\n",
    "\n",
    "# Remove numbers\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[0-9]', \"\").tolist()\n",
    "\n",
    "# Remove punctuation\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[^\\\\w\\\\s]', \"\")\n",
    "\n",
    "# Remove the fill-in-the-blank blanks\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[_]+', \"\")\n",
    "\n",
    "# Remove extra white space\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[\\\\s]+', \" \")\n",
    "\n",
    "# Make a list containing the text message strings\n",
    "txtMsgStringList = txtLibrary.txt.tolist()\n",
    "\n",
    "# Tokenize\n",
    "txtMsgTokens = [nltk.tokenize.word_tokenize(x) for x in txtMsgStringList]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words  = set(nltk.corpus.stopwords.words('english'))\n",
    "txtMsgTokens = [[w for w in text if not w in stop_words] for text in txtMsgTokens]\n",
    "\n",
    "# Stem tokens\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "txtMsgTokensStemmed = [[porter.stem(word) for word in text] for text in txtMsgTokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the participants and their outcomes\n",
    "\n",
    "### Parameters for ppt creation\n",
    "\n",
    "* Set the seed\n",
    "* R is the number of participants\n",
    "* `mu_true` is the list of true means for the outcomes\n",
    "* `sigma_true` is a list of the true sds for the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "random.seed(1001)\n",
    "\n",
    "# Number of participants\n",
    "R = len(txtLibrary.txtID)\n",
    "\n",
    "# Extract the true topics\n",
    "txtLibrary['topic'] = txtLibrary.txtID.astype(str).str[0]\n",
    "\n",
    "# True means and standard deviations\n",
    "trueTopicList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'P']\n",
    "trueMeansList = [-20, -15, -10, -5, 0, 5, 10, 15, 20]\n",
    "trueSDsList = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "trueTopicsAndMeansDict = dict(zip(trueTopicList, trueMeansList))\n",
    "trueTopicsAndSDsDict = dict(zip(trueTopicList, trueSDsList))\n",
    "txtLibrary['true_mean'] = txtLibrary.topic.map(trueTopicsAndMeansDict)\n",
    "txtLibrary['sigma_true'] = txtLibrary.topic.map(trueTopicsAndSDsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create participant outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtLibrary['Y'] = 0\n",
    "txtLibrary = txtLibrary.set_index('txtID')\n",
    "    \n",
    "for txtid in txtLibrary.index:\n",
    "    txtLibrary.at[txtid, 'Y'] = np.random.normal(txtLibrary.at[txtid, 'true_mean'], \n",
    "                                                 txtLibrary.at[txtid, 'sigma_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the folds\n",
    "\n",
    "There are 660 text messages. For five-fold cross validation, we'll have 132 text messages in each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = np.random.choice(txtLibrary.index, size = (5, 132), replace = False)\n",
    "\n",
    "fold1IDs = np.array(folds[0])\n",
    "fold2IDs = np.array(folds[1])\n",
    "fold3IDs = np.array(folds[2])\n",
    "fold4IDs = np.array(folds[3])\n",
    "fold5IDs = np.array(folds[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training the model\n",
    "\n",
    "### Put the document labels and tokens in a sparse-matrix-like structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message IDs\n",
    "msgIDs = [[txtLibrary.index[i] for j in txtMsgTokensStemmed[i]] for i in range(0, len(txtLibrary.index))]\n",
    "msgIDsLong = [y for x in msgIDs for y in x]\n",
    "msgIDsArray = np.array(msgIDsLong)\n",
    "\n",
    "# Text Messages\n",
    "txtTokensLong = [y for x in txtMsgTokensStemmed for y in x]\n",
    "txtTokensArray = np.array(txtTokensLong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingSet(holdOutSetIDs, msgIDsArray, txtTokensArray):\n",
    "    inTrainingLogical = np.logical_not(np.isin(msgIDsArray, holdOutSetIDs))\n",
    "    trainingTokens = txtTokensArray[inTrainingLogical]\n",
    "    trainingIDsLong = msgIDsArray[inTrainingLogical]\n",
    "    return((trainingTokens, trainingIDsLong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "### The Gibbs sampling algorithm\n",
    "\n",
    "We have the following terms:\n",
    "\n",
    "\n",
    "* $m_z$ is the number of documents in cluster $z$\n",
    "* $n_z$ is the number of words in cluster $z$\n",
    "* $n_z^w$ is the number of occurrences of word w in cluster $z$\n",
    "* $N_d$ is the number of words in document $d$\n",
    "* $N_d^w$ is the number of occurrences of word $w$ in document $d$\n",
    "* $V$ is the number of words in the vocabulary\n",
    "* $D$ is the number of documents in the corpus\n",
    "* $m_k = \\sum_{d = 1}^K \\mathbb{1}(z_d = k)$\n",
    "* $n_k^w = \\sum_{d = 1}^D \\mathbb{1}(z_d = k) N_d^w$\n",
    "* $m_{k, -d} = $ number of documents in $z$ without considering document $d$\n",
    "* $n_{k, -d}^w = \\sum_{d \\ne d^ast} \\mathbb{1}(z_d = k) N_d^k = $ number of occurrences of $w$ in $k$ without considering document $d$\n",
    "* $n_{k, -d} = $ number of words in $k$ without considering document $d$\n",
    "\n",
    "The Gibbs update equation is given by \n",
    "\n",
    "\\begin{align*}\n",
    "    P(z_{d^\\ast} = \\tilde{k} \\vert z_{-d^\\ast}, w, \\alpha, \\beta) &\\propto \\frac{\\Pi_{v = 1}^V \\Pi_{i = 1}^{N_{d^\\ast}^v} n_{\\tilde{k}, -d^\\ast}^{v} + \\beta_{\\tilde{k}, v} + i - 1}{\\Pi_{i = 1}^{N_{d^\\ast}} n_{\\tilde{k}, -d^\\ast} + \\sum_v \\beta_v + i -1} \\times \\frac{m_{\\tilde{k}, -d^\\ast} + \\alpha_{\\tilde{k}}}{D - 1 + \\sum_k \\alpha_k} \\times P(y \\vert topic = \\tilde{k})\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "    P(y \\vert topic) \\sim N(\\beta_1 + \\beta_2 \\mathbb{1}(topic = 2) + \\cdots + \\beta_K \\mathbb{1}(topic = K), \\sigma^2).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The parameters and latent variables to keep track of are the $z_d$s and $\\beta$s. The update for the supervision can be done via block Gibbs sampling. Chapter 14.2 gives the algorithm for drawing from the posterior for normal regression withthe standard noninformative prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm as described in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set is an array of arrays\n",
    "# Inner arrays are an array of each document's tokens (after stemming)\n",
    "uniqueMsgIDs = np.unique(msgIDsArray)\n",
    "trainingSetIDs = set(uniqueMsgIDs) - set(fold1IDs)\n",
    "trainingSetIDs = np.array(list(trainingSetIDs))\n",
    "txtMsgTokensStemmedInArrays = [np.array(x) for x in txtMsgTokensStemmed]\n",
    "txtMsgTokensStemmedInArrays = np.array(txtMsgTokensStemmedInArrays)\n",
    "trainingSetLogical = np.isin(uniqueMsgIDs, trainingSetIDs)\n",
    "trainingSetLogical = trainingSetLogical.tolist()\n",
    "trainingSet = txtMsgTokensStemmedInArrays[trainingSetLogical]\n",
    "\n",
    "# Algorithm parameters, hyperparameters\n",
    "K = 8\n",
    "uniqueTokens = np.unique(txtTokensArray)\n",
    "V = len(uniqueTokens)\n",
    "D = len(trainingSet)\n",
    "z = np.zeros(D)\n",
    "L = 10 # Number of samples to draw\n",
    "\n",
    "# Initial values\n",
    "m_zs = np.zeros(K)\n",
    "n_zs = np.zeros(K)\n",
    "n_z__ws = np.zeros(shape = (K, V))\n",
    "\n",
    "for d in range(1, D+1):\n",
    "    # Draw an initial topic\n",
    "    current_z = np.random.choice(range(1, K+1), 1)\n",
    "    \n",
    "    # Store the topic in z\n",
    "    z[d-1] = current_z\n",
    "    \n",
    "    # Update m_z\n",
    "    m_zs[current_z - 1] = m_zs[current_z - 1] + 1\n",
    "    \n",
    "    # Update n_z\n",
    "    n_zs[current_z - 1] = n_zs[current_z - 1] + len(trainingSet[d-1])\n",
    "    \n",
    "    # Get N_d\n",
    "    N_d = len(trainingSet[d-1])\n",
    "    \n",
    "    # Update n_z__w\n",
    "    for w in range(1, N_d+1):\n",
    "        wordNum = np.where(trainingSet[d-1][w-1] == uniqueTokens)\n",
    "        wordNum = wordNum[0]\n",
    "        n_z__ws[current_z - 1, wordNum] = n_z__ws[current_z - 1, wordNum] + sum(trainingSet[d-1] == uniqueTokens[wordNum])\n",
    "\n",
    "      \n",
    "        \n",
    "for l in range(L):\n",
    "    \n",
    "    for d in range(1, D+1):\n",
    "        # Record the current cluster of d\n",
    "        z_current = int(z[d-1])\n",
    "        m_zs[z_current - 1] = m_zs[z_current - 1] - 1\n",
    "        n_zs[z_current - 1] = n_zs[z_current - 1] - len(trainingSet[d-1])\n",
    "        \n",
    "        for w in trainingSet[d-1]:\n",
    "            wordNum = np.where(w == uniqueTokens)\n",
    "            n_z__ws[z_current -1, wordNum] = n_z__ws[z_current -1, wordNum] - len(trainingSet[d-1])\n",
    "        \n",
    "        # Calculate the sampling probabilities\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters, hyperparameters, and initial values\n",
    "\n",
    "# Parameters\n",
    "K = 8 # Number of topics\n",
    "D = len(txtLibrary.index) # Number of documents\n",
    "L = 50 # Number of Gibbs samples\n",
    "\n",
    "# Hyperparameters\n",
    "uniqueTokens = np.unique(txtTokensArray)\n",
    "uniqueMsgIDs = np.unique(msgIDsArray)\n",
    "delta = np.array([1 for i in range(len(uniqueTokens))])\n",
    "alpha = np.array([1 for i in range(K)])\n",
    "\n",
    "# Place to save the samples\n",
    "zChain = [] # topic draws\n",
    "zChainLong = []\n",
    "betaChain = [] # Regression parameters\n",
    "sigmaChain = [] # Regression variance\n",
    "\n",
    "# Initial topic values\n",
    "zInit = np.random.choice(range(1, K+1),\n",
    "                        size = D,\n",
    "                        replace = True)\n",
    "zInitLong = []\n",
    "for k, x in zip(zInit, msgIDs):\n",
    "    zInitLong.append([k for i in range(len(x))])\n",
    "zInitLong = np.array([x for y in zInitLong for x in y])\n",
    "zChainLong.append(zInitLong)\n",
    "zChain.append(zInit)\n",
    "\n",
    "# Initial regression values\n",
    "betaInit = np.array([0 for i in range(K)])\n",
    "betaChain.append(betaInit)\n",
    "sigmaInit = 1\n",
    "sigmaChain.append(sigmaInit)\n",
    "\n",
    "# Outcomes for the supervision part\n",
    "y = np.array(txtLibrary.Y)\n",
    "\n",
    "\n",
    "# Odds and ends to avoid repeat calculations\n",
    "intercept = [1 for i in range(R)] # Intercept for the design matrix\n",
    "df = (R-K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-504855201416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0ma_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muniqueTokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0ma_top\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzLong_new\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mktilde\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtTokensArray\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muniqueTokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0ma_bottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gibbs sampling\n",
    "for l in range(1, 5):\n",
    "    \n",
    "    ## Supervision\n",
    "    # Prepare the design matrix\n",
    "#     X = pd.get_dummies(zChain[l-1]) # Dummify the topics\n",
    "#     X = X.loc[:,2:] # Remove the first topic (reference)\n",
    "#     X['intercept'] = intercept # Add the intercept\n",
    "#     X = X.to_numpy()\n",
    "    \n",
    "    # Get betaHat and Vbeta\n",
    "#     Vbeta = np.linalg.inv(X.transpose() @ X)\n",
    "#     betaHat = Vbeta @ X.transpose() @ y\n",
    "    \n",
    "    # Get s^2\n",
    "#     resid = y - X @ betaChain[l-1]\n",
    "#     s2 = (1/(R - K)) * (resid.transpose() @ resid)\n",
    "    \n",
    "#     # Draw sigma^2 given y\n",
    "#     sigma_new2 = scipy.stats.invgamma.rvs(df/2, (df*s2)/2)\n",
    "#     sigma_new = math.sqrt(sigma_new2)\n",
    "#     sigmaChain.append(sigma_new)\n",
    "    \n",
    "#     # Draw new betas\n",
    "#     beta_new = np.random.multivariate_normal(betaHat, Vbeta*sigma_new)\n",
    "#     betaChain.append(beta_new)\n",
    "    \n",
    "    ## Topic Updates\n",
    "    zLong_new = np.copy(zChainLong[l-1])\n",
    "    z_new = np.copy(zChain[l-1])\n",
    "    \n",
    "    for dstar in uniqueMsgIDs:\n",
    "        \n",
    "        zLong_new[msgIDsArray == dstar] = 0\n",
    "        z_new[uniqueMsgIDs == dstar] = 0\n",
    "        probs_unnormalized = np.array([0.0 for i in range(K)])\n",
    "        \n",
    "        for ktilde in range(1, K+1):\n",
    "            \n",
    "            # Calculate a\n",
    "#             a_top = 0\n",
    "#             for token in uniqueTokens:\n",
    "#                 Ndstarv = sum(txtTokensArray[msgIDsArray == dstar] == token)\n",
    "#                 for i in range(1, Ndstarv+1):\n",
    "#                     a_top += math.log(sum((zLong_new == ktilde)*(txtTokensArray == token)) + i)\n",
    "\n",
    "#             a_bottom = 0\n",
    "#             Ndstar = sum(msgIDsArray == dstar)\n",
    "#             for i in range(1, Ndstar+1):\n",
    "#                 a_bottom += math.log(len(np.unique(txtTokensArray[zLong_new == ktilde])) + sum(delta) + i - 1)\n",
    "                \n",
    "#             a = math.exp(a_top)/math.exp(a_bottom)\n",
    "\n",
    "            # Calculate a using the alternative formulation\n",
    "            a_top = 0\n",
    "            for token in uniqueTokens:\n",
    "                a_top += math.log(sum((zLong_new == ktilde)*(txtTokensArray == token)) + delta[uniqueTokens == token])\n",
    "            \n",
    "            a_bottom = 0\n",
    "            Ndstar = sum(msgIDsArray == dstar)\n",
    "            for i in range(1, Ndstar+1):\n",
    "                a_bottom += math.log(sum(zLong_new == ktilde) + sum(delta))\n",
    "            \n",
    "            a = math.exp(a_top)/math.exp(a_bottom)\n",
    "            \n",
    "\n",
    "             \n",
    "            # Calculate b    \n",
    "            b_top = sum(z_new == ktilde) + alpha[ktilde - 1]\n",
    "            b_bottom = D - 1+ sum(alpha)\n",
    "            b = b_top/b_bottom\n",
    "\n",
    "            \n",
    "            # Calculate c\n",
    "#             c = scipy.stats.norm.pdf(y[uniqueMsgIDs == dstar], betaChain[0][K-1] + betaChain[0][ktilde - 1], sigma_new)\n",
    "#             c = c.astype('float64')\n",
    "\n",
    "#             probs_unnormalized[ktilde-1] = a*b*c\n",
    "            probs_unnormalized[ktilde-1] = a*b\n",
    "        \n",
    "        probs = probs_unnormalized/sum(probs_unnormalized)\n",
    "        \n",
    "        newTopic = np.random.choice(range(1, K+1), size = 1, p = probs)\n",
    "        z_new[uniqueMsgIDs == dstar] = newTopic\n",
    "        zLong_new[msgIDsArray == dstar] = newTopic\n",
    "        \n",
    "    zChainLong.append(np.array(zLong_new))\n",
    "    zChain.append(np.array(z_new))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Use the posterior predictive to predict topics for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.591729461978594e+50\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the causal estimands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
