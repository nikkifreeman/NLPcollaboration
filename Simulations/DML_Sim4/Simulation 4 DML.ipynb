{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short text topic modeling and double machine learning for causal inference and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Nikki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nikki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the text library\n",
    "txtLibraryPath = '/Users/Nikki/Dropbox/UNC/Causal NLP/Reback_TxtLibrary/Reback_Project Tech Support Text Message Library_NF.xlsx'\n",
    "txtLibrary = pd.read_excel(txtLibraryPath, \n",
    "                           sheet_name = \"Library\",\n",
    "                          skiprows = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Tidy up the text message library \n",
    "\n",
    "* Remove the skipped lines\n",
    "* Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtLibrary.columns = ['txtID', 'txt']\n",
    "txtLibrary = txtLibrary.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "* Make all words lower case\n",
    "* Remove numbers\n",
    "* Remove single character words (i.e., \"a\", \"i\", \"n\")\n",
    "* Remove stop words\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case\n",
    "txtLibrary.txt = txtLibrary.txt.str.lower()\n",
    "\n",
    "# Remove single character words\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('\\\\b[a-z]\\\\b', \"\")\n",
    "\n",
    "# Remove numbers\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[0-9]', \"\").tolist()\n",
    "\n",
    "# Remove punctuation\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[^\\\\w\\\\s]', \"\")\n",
    "\n",
    "# Remove the fill-in-the-blank blanks\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[_]+', \"\")\n",
    "\n",
    "# Remove extra white space\n",
    "txtLibrary.txt = txtLibrary.txt.str.replace('[\\\\s]+', \" \")\n",
    "\n",
    "# Make a list containing the text message strings\n",
    "txtMsgStringList = txtLibrary.txt.tolist()\n",
    "\n",
    "# Tokenize\n",
    "txtMsgTokens = [nltk.tokenize.word_tokenize(x) for x in txtMsgStringList]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words  = set(nltk.corpus.stopwords.words('english'))\n",
    "txtMsgTokens = [[w for w in text if not w in stop_words] for text in txtMsgTokens]\n",
    "\n",
    "# Stem tokens\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "txtMsgTokensStemmed = [[porter.stem(word) for word in text] for text in txtMsgTokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the participants and their outcomes\n",
    "\n",
    "### Parameters for ppt creation\n",
    "\n",
    "* Set the seed\n",
    "* R is the number of participants\n",
    "* `mu_true` is the list of true means for the outcomes\n",
    "* `sigma_true` is a list of the true sds for the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "random.seed(1001)\n",
    "\n",
    "# Number of participants\n",
    "R = len(txtLibrary.txtID)\n",
    "\n",
    "# Extract the true topics\n",
    "txtLibrary['topic'] = txtLibrary.txtID.astype(str).str[0]\n",
    "\n",
    "# True means and standard deviations\n",
    "trueTopicList = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'P']\n",
    "trueMeansList = [-20, -15, -10, -5, 0, 5, 10, 15, 20]\n",
    "trueSDsList = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "trueTopicsAndMeansDict = dict(zip(trueTopicList, trueMeansList))\n",
    "trueTopicsAndSDsDict = dict(zip(trueTopicList, trueSDsList))\n",
    "txtLibrary['true_mean'] = txtLibrary.topic.map(trueTopicsAndMeansDict)\n",
    "txtLibrary['sigma_true'] = txtLibrary.topic.map(trueTopicsAndSDsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create participant outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtLibrary['Y'] = 0\n",
    "txtLibrary = txtLibrary.set_index('txtID')\n",
    "    \n",
    "for txtid in txtLibrary.index:\n",
    "    txtLibrary.at[txtid, 'Y'] = np.random.normal(txtLibrary.at[txtid, 'true_mean'], \n",
    "                                                 txtLibrary.at[txtid, 'sigma_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the folds\n",
    "\n",
    "There are 660 text messages. For five-fold cross validation, we'll have 132 text messages in each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = np.random.choice(txtLibrary.index, size = (5, 132), replace = False)\n",
    "\n",
    "fold1IDs = np.array(folds[0])\n",
    "fold2IDs = np.array(folds[1])\n",
    "fold3IDs = np.array(folds[2])\n",
    "fold4IDs = np.array(folds[3])\n",
    "fold5IDs = np.array(folds[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training the model\n",
    "\n",
    "### Put the document labels and tokens in a sparse-matrix-like structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message IDs\n",
    "msgIDs = [[txtLibrary.index[i] for j in txtMsgTokensStemmed[i]] for i in range(0, len(txtLibrary.index))]\n",
    "msgIDsLong = [y for x in msgIDs for y in x]\n",
    "msgIDsArray = np.array(msgIDsLong)\n",
    "\n",
    "# Text Messages\n",
    "txtTokensLong = [y for x in txtMsgTokensStemmed for y in x]\n",
    "txtTokensArray = np.array(txtTokensLong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingSet(holdOutSetIDs, msgIDsArray, txtTokensArray):\n",
    "    inTrainingLogical = np.logical_not(np.isin(msgIDsArray, holdOutSetIDs))\n",
    "    trainingTokens = txtTokensArray[inTrainingLogical]\n",
    "    trainingIDsLong = msgIDsArray[inTrainingLogical]\n",
    "    return((trainingTokens, trainingIDsLong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "### The Gibbs sampling algorithm\n",
    "\n",
    "We have the following terms:\n",
    "\n",
    "\n",
    "* $m_z$ is the number of documents in cluster $z$\n",
    "* $n_z$ is the number of words in cluster $z$\n",
    "* $n_z^w$ is the number of occurrences of word w in cluster $z$\n",
    "* $N_d$ is the number of words in document $d$\n",
    "* $N_d^w$ is the number of occurrences of word $w$ in document $d$\n",
    "* $V$ is the number of words in the vocabulary\n",
    "* $D$ is the number of documents in the corpus\n",
    "* $m_k = \\sum_{d = 1}^K \\mathbb{1}(z_d = k)$\n",
    "* $n_k^w = \\sum_{d = 1}^D \\mathbb{1}(z_d = k) N_d^w$\n",
    "* $m_{k, -d} = $ number of documents in $z$ without considering document $d$\n",
    "* $n_{k, -d}^w = \\sum_{d \\ne d^ast} \\mathbb{1}(z_d = k) N_d^k = $ number of occurrences of $w$ in $k$ without considering document $d$\n",
    "* $n_{k, -d} = $ number of words in $k$ without considering document $d$\n",
    "\n",
    "The Gibbs update equation is given by \n",
    "\n",
    "\\begin{align*}\n",
    "    P(z_{d^\\ast} = \\tilde{k} \\vert z_{-d^\\ast}, w, \\alpha, \\beta) &\\propto \\frac{\\Pi_{v = 1}^V \\Pi_{i = 1}^{N_{d^\\ast}^v} n_{\\tilde{k}, -d^\\ast}^{v} + \\beta_{\\tilde{k}, v} + i - 1}{\\Pi_{i = 1}^{N_{d^\\ast}} n_{\\tilde{k}, -d^\\ast} + \\sum_v \\beta_v + i -1} \\times \\frac{m_{\\tilde{k}, -d^\\ast} + \\alpha_{\\tilde{k}}}{D - 1 + \\sum_k \\alpha_k} \\times P(y \\vert topic = \\tilde{k})\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "    P(y \\vert topic) \\sim N(\\beta_1 + \\beta_2 \\mathbb{1}(topic = 2) + \\cdots + \\beta_K \\mathbb{1}(topic = K), \\sigma^2).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The parameters and latent variables to keep track of are the $z_d$s and $\\beta$s. The update for the supervision can be done via block Gibbs sampling. Chapter 14.2 gives the algorithm for drawing from the posterior for normal regression withthe standard noninformative prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm as described in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set is an array of arrays\n",
    "# Inner arrays are an array of each document's tokens (after stemming)\n",
    "uniqueMsgIDs = np.unique(msgIDsArray)\n",
    "trainingSetIDs = set(uniqueMsgIDs) - set(fold1IDs)\n",
    "trainingSetIDs = np.array(list(trainingSetIDs))\n",
    "txtMsgTokensStemmedInArrays = [np.array(x) for x in txtMsgTokensStemmed]\n",
    "txtMsgTokensStemmedInArrays = np.array(txtMsgTokensStemmedInArrays)\n",
    "trainingSetLogical = np.isin(uniqueMsgIDs, trainingSetIDs)\n",
    "trainingSetLogical = trainingSetLogical.tolist()\n",
    "trainingSet = txtMsgTokensStemmedInArrays[trainingSetLogical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters, hyperparameters\n",
    "K = 8\n",
    "uniqueTokens = np.unique(txtTokensArray)\n",
    "V = len(uniqueTokens)\n",
    "D = len(trainingSet)\n",
    "z = np.zeros(D)\n",
    "L = 1000 # Number of samples to draw\n",
    "delta = np.array([1 for i in range(len(uniqueTokens))])\n",
    "alpha = np.array([1 for i in range(K)])\n",
    "\n",
    "# Initial values\n",
    "m_zs = np.zeros(K)\n",
    "n_zs = np.zeros(K)\n",
    "n_z__ws = np.zeros(shape = (K, V))\n",
    "\n",
    "for d in range(1, D+1):\n",
    "    # Draw an initial topic\n",
    "    current_z = np.random.choice(range(1, K+1), 1)\n",
    "    \n",
    "    # Store the topic in z\n",
    "    z[d-1] = current_z\n",
    "    \n",
    "    # Update m_z\n",
    "    m_zs[current_z - 1] = m_zs[current_z - 1] + 1\n",
    "    \n",
    "    # Update n_z\n",
    "    n_zs[current_z - 1] = n_zs[current_z - 1] + len(trainingSet[d-1])\n",
    "    \n",
    "    # Get N_d\n",
    "    N_d = len(trainingSet[d-1])\n",
    "    \n",
    "    # Update n_z__w\n",
    "    for w in np.unique(trainingSet[d-1]):\n",
    "        wordNum = np.where(w == uniqueTokens)\n",
    "        wordNum = wordNum[0]\n",
    "        n_z__ws[current_z - 1, wordNum] = n_z__ws[current_z - 1, wordNum] + sum(trainingSet[d-1] == w)\n",
    "zChain = [z]\n",
    "   \n",
    "        \n",
    "for l in range(L):\n",
    "    \n",
    "    for d in range(1, D+1):\n",
    "        # Record the current cluster of d\n",
    "        z_current = int(z[d-1])\n",
    "        m_zs[z_current - 1] = m_zs[z_current - 1] - 1\n",
    "        n_zs[z_current - 1] = n_zs[z_current - 1] - len(trainingSet[d-1])\n",
    "        \n",
    "        for w in np.unique(trainingSet[d-1]):\n",
    "            \n",
    "            wordNum = np.where(w == uniqueTokens)\n",
    "            wordNum = wordNum[0]\n",
    "            n_z__ws[z_current -1, wordNum] = max(n_z__ws[z_current -1, wordNum] - len(trainingSet[d-1] == w), 0)\n",
    "\n",
    "        \n",
    "        # Calculate the sampling probabilities\n",
    "        probs = np.zeros(shape = K)\n",
    "        for ktilde in range(1, K+1):\n",
    "            \n",
    "            a_num = 1\n",
    "            for token in np.unique(trainingSet[d-1]):\n",
    "                wordNum = np.where(token == uniqueTokens)\n",
    "                wordNum = wordNum[0]\n",
    "                \n",
    "                for j in range(1, sum(trainingSet[d-1] == token) + 1):\n",
    "                    a_num = a_num * (n_z__ws[ktilde - 1, wordNum] + delta[wordNum] + j - 1)\n",
    "                    \n",
    "            a_denom = 1\n",
    "            for i in range(1, len(trainingSet[d-1])+1):\n",
    "                a_denom = (n_zs[ktilde - 1] + sum(delta) + i -1)\n",
    "                \n",
    "            a = a_num/a_denom\n",
    "            \n",
    "            b_num = max(m_zs[ktilde-1], 0) + alpha[ktilde-1]\n",
    "            \n",
    "            b_denom = D - 1 + sum(alpha)\n",
    "            \n",
    "            b = b_num/b_denom\n",
    "            \n",
    "            probs[ktilde - 1] = a*b\n",
    "        \n",
    "        # Draw a new topic\n",
    "        new_z = np.random.choice(range(1, K+1), p = probs/sum(probs))\n",
    "        \n",
    "        z[d-1] = new_z\n",
    "        \n",
    "        \n",
    "        m_zs[new_z - 1] = m_zs[new_z - 1] + 1\n",
    "        n_zs[new_z - 1] = n_zs[new_z - 1] + len(trainingSet[d-1])\n",
    "        \n",
    "        for w in np.unique(trainingSet[d-1]):\n",
    "            \n",
    "            wordNum = np.where(w == uniqueTokens)\n",
    "            wordNum = wordNum[0]\n",
    "            n_z__ws[new_z -1, wordNum] = n_z__ws[new_z -1, wordNum] + len(trainingSet[d-1] == w)\n",
    "    \n",
    "    zChain.append(np.array(z))\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zChainDf = pd.DataFrame(zChain)\n",
    "zChainDf.columns = trainingSetIDs\n",
    "zChainDf.to_csv('zchain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Use the posterior predictive to predict topics for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the causal estimands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
